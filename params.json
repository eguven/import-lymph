{"name":"Import-lymph","tagline":"An introduction to http://lymph.io","body":"<img alt=\"build status\" align=\"right\" src=\"https://travis-ci.org/mamachanko/import-lymph.svg\">\r\n\r\n# `import lymph`\r\n\r\nWelcome to an introduction to [lymph](http://lymph.io). Lymph is a framework\r\nfor writing services in Python.\r\n\r\n## Playground\r\n\r\nWe've got a [vagrant](http://vagrantup.com) box for you with lymph services\r\nrunning inside. We suggest you use it, but local setup should be straightfoward\r\nas well.\r\n\r\nThe box provisions with all tooling and code ready for your perusal. It has\r\nboth [Zookeeper](http://zookeeper.apache.org/) and\r\n[RabbitMQ](https://www.rabbitmq.com/) running inside. Services and lymph's\r\ntooling can be explored within pre-configured tmux sessions.\r\n\r\nThe only prerequisits are\r\n[vagrant](http://docs.vagrantup.com/v2/installation/index.html) and\r\n[ansible](http://docs.ansible.com/intro_installation.html) on your host\r\nmachine.\r\n\r\nGetting hands-on is a matter of running:\r\n\r\n```shell\r\ngit clone git@github.com:mamachanko/import-lymph.git\r\ncd import-lymph\r\nvagrant up && vagrant ssh\r\n```\r\n\r\nYou will be prompted for your root password half-way through `vagrant up`\r\nbecause we use NFS to share files.\r\n\r\nOnce inside the box, the `motd` contains more information. You can directly\r\nfollow all the examples shown in this introduction.\r\n\r\n<img align=\"left\" src=\"https://github.com/mamachanko/import-lymph/blob/master/images/motd.png\" width=\"49%\">\r\n<img align=\"right\" src=\"https://github.com/mamachanko/import-lymph/blob/master/images/mux.png\" width=\"49%\">\r\n\r\nLet's start then!\r\n\r\n## Stop trying to glue your services together\r\n> An introduction to lymph by Alejandro Castillo & Max Brauer\r\n\r\nWe'd like to introduce you to _lymph_, a framework for\r\nwriting services in Python. With lymph you can write services with almost no\r\nboilerplate. But let's introduce ourselves first.\r\n\r\nWe're [Delivery Hero](http://deliveryhero.com), a holding of online\r\nfood-ordering services. We're located in Berlin. We operate in 34 countries\r\nand growing.\r\n\r\nLet me explain the concept of online food-ordering to those who're unfamiliar\r\nwith it. However, I seriously doubt this is news to anyone. The concept is\r\nsimple:\r\n\r\n* get hungry\r\n* go online\r\n* search for restaurants\r\n* compile your order\r\n* pay online\r\n* wait for delivery\r\n\r\nBasically, it's e-commerce with very grumpy customers.\r\n\r\nHow's this introduce structured? Let's briefly go over the topics of this\r\nintroduction:\r\n\r\n1. We're going to explain where we're coming from and why we have given birth to a(nother) framework\r\n1. We'll look at code as fast possible\r\n1. We'll run services and progressively add new services to explore forms of communication and the tooling of lymph\r\n1. We'll give you a brief overview of further features and talk about how lymph does things under the hood\r\n1. We'll touch on similar technology and see how it relates to lymph (Nameko in particular)\r\n1. We'll talk future plans for lymph and its ecosystem\r\n1. That's it :)\r\n\r\nBefore we go ahead, here's a little disclaimer. For the sake of this\r\nintroduction, we assume that you're familiar with the concept of services. We\r\nassume you're familiar with monoliths. We assume that you are familiar with\r\nwhen and why to use either and even more so when not. We will not discuss the\r\ndifferences between the two. We won't talk about how services might safe your\r\ndevelopment teams or your business. Neither will we talk about sophisticated\r\nnetworking topologies, Docker, \"microservices\", ...\r\n\r\nBut what we're going to talk about is lymph. By the end of the talk you should\r\nunderstand what lymph can and cannot do and why that's cool. If we achieved\r\nthat we'd consider ourselves succesful.\r\n\r\n### Why write a framework?\r\n\r\nOur initial situation was the classic one. We had a massive Django monolith.\r\nWe weren't moving fast at all. We've had trouble finding rhythm for a growing\r\nnumber of teams and developers. Teams were blocked by other teams. The code\r\nbase was a big bowl of legacy spaghetti. We've had issues scaling.\r\n\r\nBasically, what comes to mind is the textbook situation: \"my monolith hurts, i\r\nwant services\". The idea of a service-oriented architecture became increasingly\r\nreasonable and attractive to us.\r\n\r\nAnother aspect is our heterogenic product landscape. While most countries'\r\nwebsites work the same(\"order food online\") they all differ one way or another.\r\nModularity, extensibility, reuseability, scaleability... are key for us.\r\n\r\nStill, the obvious question is: _\"why write another framework?\"_. The answer is\r\nalmost as obvious. At that time there was nothing that fits our needs. We wanted\r\nto work with services but we wanted some very specific things:\r\n\r\n* We are mainly Python-powered and we wanted to continue rolling with it.\r\n  Language-agnosticity is great, but not really important for us(yet).\r\n* Running and testing services should be easy.\r\n* Developers shouldn't have to worry about registering and discovering services.\r\n* You don't have to serialize data for sending it over the wire. Neither do you\r\n  want to manually deserialize it.\r\n* Requesting remote services should be almost like in-process calls.\r\n* Developers shouldn't have to deal with event loops nor know about any\r\n  transport mechanism.\r\n* Configuring services should be straightforward and flexible.\r\n* Scaling should be easy, but transparent to clients.\r\n* Services should speak HTTP.\r\n* We don't want boilerplate if it can be avoided.\r\n\r\nTo sum things up, we wanted as little in the way as possible between\r\ndevelopers and functionality they want to serve.\r\n\r\nYou might say _\"hey, use [nameko](https://nameko.readthedocs.org)\"_. But nameko\r\nhas not come to our attention until few months ago. Neither was it mature\r\nenough to be adopted way back then. Later we're going to talk technologies\r\nsimilar to nameko and lymph.\r\n\r\nTaking all these things into consideration, rolling our own thing was actually\r\nreasonable. And to not suspend any further, say hello to\r\n[lymph](http://lymph.io). Hopefully, you're itching to see what a service looks\r\nlike in lymph. If you know nameko you won't be surprised though.\r\n\r\nWe'll break the ice by running and playing around with services. We'll slowly\r\nprogress through lymph's features, service by service. This is a good time to\r\nboot the vagrant box.\r\n\r\n### Hands-on\r\n\r\n#### The greeting service\r\n\r\n[This](https://github.com/mamachanko/import-lymph/blob/master/services/greeting.py)\r\nis what a simple greeting service looks like in lymph. Its interface is one RPC\r\nmethod called `greet` which takes a name, prints it, emits an event(containing\r\nthe name in the body) and returns a greeting for the given name.\r\n\r\n```python\r\nimport lymph\r\n\r\n\r\nclass Greeting(lymph.Interface):\r\n\r\n    @lymph.rpc()\r\n    def greet(self, name):\r\n        print('Saying hi to %s' % name)\r\n        self.emit('greeted', {'name': name})\r\n        return u'Hi, %s!' % name\r\n```\r\n\r\nAll we need to do to make things happen is to inherit from `lymph.Interface`\r\nand decorate RPC methods with `@lymph.rpc()`. Lastly, we've got the interface's\r\n`emit()` function to our disposal which dispatches events in the event system.\r\n\r\nLet's jump on the shell and play with it.\r\n\r\n``` shell\r\n» mux start greeting\r\n```\r\n\r\nWhat you see here is a tmux session with two panes. On the right-hand side you\r\nsee the greeting service being run with lymph's `instance` command. On the\r\nleft-hand side you see a plain shell on which we'll explore lymph's tooling.\r\n\r\nEvery time you want to run an instance of a servcie you need to point lymph to\r\nits configuration. The configuration tells lymph about the interface's name and\r\nwhere it can import it from. Since lymph imports the interface, its modulea\r\nneeds to be on the `PYTHONPATH`. We can make this happen with `export\r\nPYTHONPATH=services` in our example. But worry not, the tmuxinator sessions\r\ntake care of it for you. Our service's configuration looks like\r\n[this](https://github.com/mamachanko/import-lymph/blob/master/conf/greeting.yml):\r\n\r\n```yaml\r\ninterfaces:\r\n    Greeting:\r\n        class: greeting:Greeting\r\n```\r\n\r\nThe `lymph instance` command receives the path to this file.\r\n\r\nOne of the first things we considered when building lymph was the tooling. We\r\nthink we managed to get some very nice tooling built around it to make\r\ndevelopment of services easier.\r\n\r\nSo what tooling is available? `lymph list` will tell us.\r\n\r\n``` shell\r\n» lymph list\r\nnode       Run a node service that manages a group of processes on the same machine.\r\nshell      Open an interactive Python shell locally or remotely.\r\nhelp       Display help information about lymph.\r\ninstance   Run a single service instance (one process).\r\nlist       List available commands.\r\ninspect    Describes the RPC interface of a service\r\nrequest    Send a request message to some service and output the reply.\r\ndiscover   Show available services.\r\nsubscribe  Prints events to stdout.\r\ntail       Stream the logs of one or more services.\r\nemit       Manually emits an event.\r\n```\r\n\r\nYou see there's plenty of commands available to interact with services. Worry\r\nnot, we'll explore them one by one.\r\n\r\nTo begin with let's assert that an instance of the echo service is running.\r\nWe'll use lymph's `discover` command.\r\n\r\n```shell\r\n» lymph discover\r\nGreeting [1]\r\n```\r\n\r\nAs you can see, one instance is running indeed (`Greeting [1]`). But what did\r\njust happen?  When the service started in the right-hand pane it registered\r\nitself with Zookeeper by providing its name and address. When we ran the\r\ndicovery command found that entry. If we stopped the service, it'd\r\nunregistered itself and the discovery command would say that no instances are\r\nrunning.\r\n\r\nLet's pretend we don't know what the greeting service has to offer. We'd like\r\nto find out though:\r\n \r\n```shell\r\n» lymph inspect Greeting\r\nRPC interface of Greeting\r\n\r\nrpc Greeting.greet(name)\r\n\r\nrpc lymph.get_metrics()\r\n\r\nrpc lymph.inspect()\r\n         Returns a description of all available rpc methods of this service\r\n\r\nrpc lymph.ping(payload)\r\n\r\nrpc lymph.status()\r\n```\r\n\r\nWe see that the interface of the greeting service is composed of inherited\r\nmethods(from `lymph.Interface`) and our `greet` method. Let's excercise the\r\n`greet` method. We'll use lymph's `request` command. Therefore, we have to\r\nprovide the service name, the name of the method and the body of the request as\r\nJSON. What we expect to see is the echo service to return the text as is, but\r\nit should also print it and emit an event.\r\n\r\n```shell\r\n» lymph request Greeting.greet '{\"name\": \"Joe\"}'\r\nu'Hi, Joe!'\r\n```\r\n\r\nThe response to the RPC request is `'Hi, Joe!'`. It's as expected and the\r\nservice printed the name. But what exactly did just happen? When we issued the\r\nrequest lymph did the following:\r\n\r\n1. looked up the address of the greeting service in Zookeeper\r\n1. serialized the request body with MessagePack\r\n1. sent it over the wire via zeromq\r\n1. the service received the request\r\n1. the service deserialized the request using MessagePack\r\n1. the service performed the heavy computation to produce the desired greeting for Joe\r\n1. the response was once more serialized(MessagePack) and sent back(ZeroMQ) to the requestee\r\n1. the requestee(our shell client) deseria... and printed\r\n\r\nWhoi! That's a lot. This is where lymph lives up to this introduction's claim.\r\nThis is all the glue that lymph is.\r\n\r\nOur single service is rather boring though. It's also pretty lonely. Nobody\r\nlistens to its events. Here comes a listener.\r\n\r\n#### The listen service\r\n\r\nThe [listen\r\nservice](https://github.com/mamachanko/import-lymph/blob/master/services/listen.py)\r\nlistens to greeting's events Again, it's a lymph service(we inherit from\r\n`lymph.Interface`). However, there's nothing but one method which is subscribed\r\nto `greeted` events. It simply prints the greeted name contained in the event's\r\nbody. Everytime an event of this type occurs exactly once instance of the\r\nlisten service will consume it.\r\n\r\n```python\r\nimport lymph\r\n\r\n\r\nclass Listen(lymph.Interface):\r\n\r\n    @lymph.event('greeted')\r\n    def on_greeted(self, event):\r\n        print('Somebody greeted %s' % event['name'])\r\n```\r\n\r\nLet's excercise our services combination. This time round, though, we'll run\r\ntwo instances of the greeting service and one instance of the listen service.\r\n\r\nThe [listen service's\r\nconfiguration](https://github.com/mamachanko/import-lymph/blob/master/conf/listen.yml)\r\nis no different from the one before.\r\n\r\n``` shell\r\n» mux start greeting-listen\r\n```\r\n\r\nAgain, we see a tmux session. On the right you find two instances of the\r\ngreeting service followed by an instance of the listen service.\r\n\r\nWe should find them registered correctly.\r\n\r\n``` shell\r\n» lymph discover\r\nGreeting [2]\r\nListen [1]\r\n```\r\n\r\nAnd, indeed, they list correctly. Remember, this comes from Zookeeper.\r\n\r\nLet's emit a `greeted` event in the event system to assert whether the listen\r\nservice listens to it. We'll use lymph's `emit` command. We're expecting the\r\nlisten service to print the name field from the event body.\r\n\r\n```\r\n» lymph emit greeted '{\"name\": \"Joe\"}'\r\n```\r\n\r\nNice. That worked. The listen service printed as expected. But what did just\r\nhappen exactly? Our shell client serialized the event body using MessagePack and\r\npublished it to RabbitMQ with the `greeted` event type. Then it returned. The\r\nlisten service on the other hand is subscribed to these events in RabbitMQ.\r\nOnce we published it consumed the event, deserialized it and processed it. As\r\nan outcome it printed.\r\n\r\nIf there were several instances of the listen service only one of them would've\r\nconsumed the event. That's singlecast. Lymph can also broadcast to all\r\ninstances of a service.\r\n\r\nKeep in mind that if another service would've subscribed to this event as well,\r\nonce instance of it would've also consumed the event. Yet, only one instance of\r\neach subscribed service. That's the pub-sub communication pattern. Also, we\r\ncould emit any random event and our shell client would return, e.g. `lymph emit\r\nhi '{}'`. Publishers don't know about subcribers or if they exist at all.\r\n\r\nReturning to our example, when we do RPC requests we expect the greeting\r\ninstances to respond in round-robin fashion while the listen instance should\r\nrect to all occuring events.\r\n\r\n``` shell\r\n» lymph request Greeting.greet '{\"name\": \"Joe\"}'\r\nu'Hi, Joe!'\r\n```\r\n(do this repeatedly, until both greeting instances have responded)\r\n\r\nAs you see, our expectations are met. Lymph takes care of picking one of the\r\ninstances from Zookeeper. That's client-side load-balancing.\r\n\r\nIf we were to run several instances of the listen services, each event would be\r\nconsumed by exactly once instance. However, lymph allows to broadcast events as\r\nmentioned above.\r\n\r\nFinally, since it's 2015, no talk would be complete without talking about HTTP.\r\nLet's add a web service to the mix. Let's say we wanted to expose the greeting\r\nfunctionality via an HTTP API. Lymph has a class for that.\r\n\r\n#### The web service\r\n\r\n[This](https://github.com/mamachanko/import-lymph/blob/master/services/web.py)\r\nis the Web service. It subclasses lymph's `WebServiceInterface`. In this case\r\nwe're not exposing RPC methods, emitting not listening to events. However, we\r\nconfigure a Werkzeug URL map as a class attribute. We've added one endpoint and\r\na handler for it: `/greet`. The handler receives a Werkzeug request object.\r\n\r\nWebservice are very powerful as they help to expose our services capabilities\r\nto the world in the internet's language: HTTP.\r\n\r\n```python\r\nfrom lymph.web.interfaces import WebServiceInterface\r\nfrom werkzeug.routing import Map, Rule\r\nfrom werkzeug.wrappers import Response\r\n\r\n\r\nclass Web(WebServiceInterface):\r\n\r\n    url_map = Map([\r\n        Rule('/greet', endpoint='greet'),\r\n    ])\r\n\r\n    def greet(self, request):\r\n        \"\"\"\r\n        handles:\r\n            /greet?name=<name>\r\n        \"\"\"\r\n        name = request.args['name']\r\n        print('About to greet %s' % name)\r\n        greeting = self.proxy('Greeting').greet(name=name)\r\n        return Response(greeting)\r\n```\r\n\r\nThe greet handler expects a name to be present in the query string. It calls\r\nthe greeting service via the `self.proxy`, returns the result in the\r\nresponse and it prints.\r\n\r\nMind, that we're not validating the request method nor anything else.\r\n\r\nRun it or it didn't happen, they say. We'll bring up an instance of each of\r\nservices now.\r\n\r\nOnce more, the [web service's\r\nconfiguration](https://github.com/mamachanko/import-lymph/blob/master/conf/web.yml)\r\nis no different from the ones we looked at before.\r\n\r\n``` shell\r\n» mux start all\r\n```\r\n\r\nOn the right you can see an instance of every service: web, greeting and\r\nlisten.\r\n\r\nOnce again, they should have registered correctly:\r\n\r\n``` shell\r\n» lymph discover\r\nWeb [1]\r\nGreeting [1]\r\nListen [1]\r\n```\r\n\r\nLet's hit our web service and see how the request penetrates our service\r\nlandscape. We should see all service print something. The web service is\r\nlistening at the default port 4080. We're using `httpie` to excercise the\r\nrequest:\r\n\r\n```\r\n» http localhost:4080/greet?name=Joe\r\nHTTP/1.1 200 OK\r\nContent-Length: 8\r\nContent-Type: text/plain; charset=utf-8\r\nDate: Wed, 08 Jul 2015 20:51:30 GMT\r\nX-Trace-Id: 3adc102707f745239efb2837c1877f59\r\n\r\nHi, Joe!\r\n\r\n```\r\n\r\nThe response looks good and all services should have performed accordingly.\r\n\r\n#### Lymph's development server\r\n\r\nYet, when developing locally you seldomly want to run all of your services\r\nwithin different shells or tmux panes. Lymph has its own development server\r\nwhich wraps around any number of services with any number of instances.\r\nTherefore, we'll have to configure which services to run and with how many\r\ninstances in [`.lymph.yml`](.lymph.yml):\r\n\r\n```yaml\r\ninstances:\r\n    Web:\r\n        command: lymph instance --config=conf/web.yml\r\n        numprocesses: 2\r\n\r\n    Greeting:\r\n        command: lymph instance --config=conf/greeting.yml\r\n        numprocesses: 3\r\n\r\n    Listen:\r\n        command: lymph instance --config=conf/listen.yml\r\n        numprocesses: 4\r\n\r\n\r\nsockets:\r\n    Web:\r\n        port: 4080\r\n```\r\n\r\n(Since we run several instances of our web service we have to configure a\r\nshared sockets for it.)\r\n\r\nMind that in our case `command` specifies lymph instances but this could also\r\nbe any other service you need, e.g. Redis.\r\n\r\nLet's bring them all up.\r\n\r\n``` shell\r\n» mux start all\r\n```\r\n\r\nOnce more, we find ourselves inside a tmux session with `lymph node` running in\r\nthe top-right pane. Below that you see `lymph tail` running which allows us to\r\nfollow the logs of any number of services. But first, let's check how many\r\ninstances are running:\r\n\r\n``` shell\r\n» lymph discover\r\nWeb [2]\r\nGreeting [3]\r\nListen [4]\r\n```\r\n\r\nThat's a good number. Once we feed a request into the cluster we should see\r\nprint statements and logs appearing.\r\n\r\n``` shell\r\n» http localhost:4080/greet?name=Joe\r\nHTTP/1.1 200 OK\r\nContent-Length: 8\r\nContent-Type: text/plain; charset=utf-8\r\nDate: Wed, 08 Jul 2015 20:51:30 GMT\r\nX-Trace-Id: 3adc102707f745239efb2837c1877f59\r\n\r\nHi, Joe!\r\n\r\n```\r\n\r\nWithin the `node` pane we should the see following haiku-esque sequence of print\r\nstatements:\r\n\r\n```shell\r\n» lymph node\r\nAbout to greet Joe\r\nSaying hi to Joe\r\nSomebody greeted Joe\r\n```\r\n\r\nWithin the `tail` pane though, there's a lot going on. You would find an even\r\nbigger mess the more services and instances you run and the more intricated\r\nyour patterns of communication become. Sometimes you wonder \"where did my\r\nrequest go?\". Lymph helps you though with `trace_id`s. Every request that\r\nappears in our cluster which doesn't have a _trace id_ assigned yet gets one.\r\nTrace ids get fowarded with every RPC and event.\r\n\r\nWe are able to corellate all log statements in our cluster to that one HTTP\r\nrequest. In fact the web service returns the the trace id in the `X-Trace-Id`\r\nheader. If you check the logs within the tail pane you should see that all logs\r\ncan be correlated with that trace id. And indeed we see the same `trace_id`\r\nacross our service instances for every incoming request:\r\n\r\n<img src=\"https://github.com/mamachanko/import-lymph/blob/master/images/tail.png\" width=\"98%\">\r\n\r\nWe've covered most of the available tooling. You should have a pretty good idea\r\nhow to interact with your services now.\r\n\r\nThere's one command we haven't tried yet. That's `lymph subscribe`. It is being\r\nleft to the reader as an excercise.\r\n\r\n### More built-in features\r\n\r\n#### Testing\r\n\r\nLet's talk about features which go beyond CLI tooling. Testing services is\r\ncrucial for development. Have a look at our services [`tests`](tests.py) to get\r\nan idea of lymph testing utilities. The tests showcase how you would tests\r\nthese three varieties of services. You can run the tests with:\r\n\r\n```shell\r\nPYTHONPATH=services nosetests --with-lymph\r\n```\r\n\r\n#### Configuration\r\n\r\nEarlier, we have mentioned that ease of configuration was a concern for lymph.\r\nThere's an API to deal configuration files. Its thin abstraction over the\r\nactual YAML files but gives you the freedom to instantiate classes right from\r\nthe config. This gives you the freedom to configure services with a minial\r\namount of code. Custom configuration can be processed by overriding lymph\r\ninterface's `apply_config(self, config)` hook.\r\n\r\nLet's say your `Service` is supposed have a `cache_client`. This client should\r\nbe fully configurable and the ramifications of instantiating should not be the\r\nservice's concern. We could configure our service to run with a redis like so:\r\n\r\n(for this we assume that both `redis` and `memcache` are set in your\r\n`/etc/hosts` to point to the respective IPs. that'd be `127.0.0.1` in most\r\ncases. but `redis` and `memcache` read nicer.)\r\n\r\n```yaml\r\n# conf/service_redis.yaml\r\ninterfaces:\r\n    Service:\r\n        class: service:Service\r\n\r\n        cache_client:\r\n            class: redis.client:StrictRedis\r\n            host: redis\r\n            port: 6379\r\n            db: 1\r\n```\r\n\r\nAs you can see the config contains all information we need to instantiate the\r\nredis client's class. Let's make use of the config API then and configure\r\nour service in the `apply_config(self, config)` hook:\r\n\r\n```python\r\n# service/service.py\r\nimport lymph \r\n\r\n\r\nclass Service(lymph.Interface):\r\n\r\n    def apply_config(self, config):\r\n        super(Service, self).apply_config(config)\r\n        self.cache_client = config.get_instance('cache_client')\r\n```\r\n\r\nAs you see it takes only one line to configure the service. Running it is a matter\r\nof pointing to the desired config file:\r\n\r\n```shell\r\n» lymph instance --config=conf/service_redis.yml\r\n```\r\n\r\nWe could have other instances running on top of memcache though. Like so:\r\n\r\n```yaml\r\ninterfaces:\r\n    Service:\r\n        class: service:Service\r\n\r\n        cache_client:\r\n            class: pymemcache.client:Client\r\n            server:\r\n              - memcache\r\n              - 11211\r\n```\r\n\r\nAgain, you run it by pointing it to the desired config file:\r\n\r\n```shell\r\n» lymph instance --config=conf/service_memcache.yml\r\n```\r\n\r\nYou can also share instance of such classes over instances of your services.\r\nAll service instances [share the same zookeeper client\r\ninstance](https://github.com/deliveryhero/lymph/blob/master/conf/sample-node.yml#L9)\r\nby default. Or you read from [environment\r\nvariables](http://lymph.readthedocs.org/en/latest/configuration.html#environment-variables).\r\n\r\n#### Up and down\r\n\r\nYou may want to set the stage when bringing up your service. Performing\r\nclean-up tasks when shutting down is just as likely. Therefore you may override\r\nthe `on_start(self)` and `on_stop(self)` hooks.  For the `Service` from the\r\nprevious section we could do:\r\n\r\n```python\r\n    def on_start(self):\r\n        super(Service, self).on_start()\r\n        print('started with client: %s' % self.cache_client)\r\n\r\n    def on_stop(self):\r\n        print('stopping...')\r\n        super(Service, self).on_stop()\r\n```\r\n\r\n#### Futures\r\n\r\nClassic RPC calls block until the response is received. A deferred RPC call\r\nmechanism is implemented in case you wish to consume the RPC response later, or\r\nsimply ignore it. Lymph's RPC implementation allows to defer calls:\r\n\r\n```python\r\nproxy('Greeting').greet.defer(name=u'John')  # non-blocking\r\n```\r\n\r\n#### Metrics\r\n\r\nStandard process metrics are being collected out of the box and exposed via an\r\ninternal API. It is possible to collect and expose custom metrics.\r\n\r\n#### Plugins\r\n\r\nLymph has a plugin system which allows you to register code for certain hooks,\r\ne.g. `on_error`, `on_http_request`, `on_interface_installation` etc. A [New\r\nRelic](https://github.com/deliveryhero/lymph/blob/master/lymph/plugins/newrelic.py)\r\nand a\r\n[Sentry](https://github.com/deliveryhero/lymph/blob/master/lymph/plugins/sentry.py)\r\nplugin are shipped as built-ins. You can also register your own hooks.\r\n\r\n#### CLI extensions\r\n\r\nLastly, CLIs are pluggable as well. Have a look at [`lymph\r\ntop`](https://github.com/mouadino/lymph-top) as an example.\r\n\r\n### Under the hood\r\n\r\nHow does lymph do things under the hood? It isn't black magic. This section\r\nactually turned out to be much shorter than expected.\r\n\r\nLymph depends on Zookeeper for service registry. Zookeeper is a distributed\r\nkey-value store. Once a service instance is being started it registers itself\r\nwith Zookeper providing its address and name.  Once it's being stopped it\r\nunregisters itself. When you send a request to another service, lymph gets all\r\ninstances' addresses and routes the request. That means, lymph does client-side\r\nload balancing. Request\r\n[round-robin](https://github.com/deliveryhero/lymph/blob/master/lymph/core/rpc.py#L140-155)\r\nover the instances of a service. The request itself is being serialized with\r\n[MessagePack](http://msgpack.org/) and send over the wire using\r\n[ZeroMQ](http://zeromq.org/).\r\n\r\nLymph's pub-sub event system is powered by\r\n[RabbitMQ](https://www.rabbitmq.com/).  That means any valid topic exchange\r\nrouting key is valid lymph event type. Lymph also allows to broadcast and delay\r\nevents.\r\n\r\nEvery service instance is a single Python process which handles requests and\r\nevents via of greenlets. Lymph uses [gevent](http://www.gevent.org/) for this.\r\n\r\nLymph uses [werkzeug](http://werkzeug.pocoo.org/) to handle everything WSGI and\r\nHTTP.\r\n\r\n### Related frameworks and tech\r\n\r\nRight now it seems as if both [nameko](https://nameko.readthedocs.org/) and\r\nlymph are the only Python service frameworks that exist. This is true for the\r\nlevel of integration and self-contained tooling at least. Obviously, they are\r\ndifferent frameworks by different authors having been implemented\r\nindependently. Nonetheless, they do share some striking characteritics. To us,\r\nthese similarities validate our assumptions. It is worth mentioning that nameko\r\ndoesn't do service registry explicitly though. Nameko achieves this by using\r\nRabbitMQ for both events and RPC. It'd be tedious to compare them both in\r\ndetail but it is highly recommended to have a look at nameko as well.\r\n\r\nMost other similar technologies aren't either Python-specific or provide\r\nspecific solutions for RPC for instance like [zerorpc](http://www.zerorpc.io/).\r\n\r\nThings that are worth mentioning though are\r\n[cocaine](https://cocaine.readthedocs.org/en/latest/),\r\n[spread](http://www.spread.org/index.html),\r\n[circuits](http://circuitsframework.com/), ...\r\n\r\n### Future\r\n\r\nWe intend to grow the little ecosystem around lymph. While lymph will stay the\r\ncore framework, we're already in the process of developing complementary\r\nlibraries for writing special-purpose services, e.g. for storage and business\r\nprocess. Naturally, we'll open-source them once they have matured well enough.\r\n\r\nLastly, we'd like to mention\r\n[distconfig](https://github.com/deliveryhero/distconfig) which is a Python\r\nlibrary for managing shared state, i.e. configuration, feature switches etc.\r\nDistconfig is intended to complement lymph.\r\n\r\n### Outroduction\r\n\r\nThank you very much for you attention! We're done. We hope you enjoyed this\r\nlittle introduction. If you feel that the ice is broken we'd consider ourselves\r\nsuccessful. If you should have questions feel free to reach out in our IRC\r\nchannel `#lymph` on _freenode.net_. Naturally, we'd appreciate PRs and issues\r\non GitHub.\r\n\r\nIf you should spot any unclarities or errata within this introduction, you're\r\nvery welcome to point them out at\r\n[github.com/mamachanko/import-lymph](https://nameko.readthedocs.org/).\r\n","google":"UA-64656787-1","note":"Don't delete this file! It's used internally to help with page regeneration."}